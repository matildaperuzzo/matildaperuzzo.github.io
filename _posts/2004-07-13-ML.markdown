---
layout: default
modal-id: 6
img: ML.png
alt: Machine Learning

title1: Reinforcement Learning with Tetris
description1: </p>
        <div class="cv-section">
        <p>Fraser plays Tetris is a reinforcement learning project based on python libraries PyGame and PyTorch. The algorithm is based on a project by <a href="https://github.com/patrickloeber/python-fun" target="_blank">patrickloeber</a> on GitHub which revolved around the classic game Snake. I modified the code to work for Tetris, here are some of my findings.</p>
        <p>Find my code <a href="https://github.com/matildaperuzzo/fraser-plays-tetris" target="_blank">here.</a></p>

        <p style="min-height:20px"></p>
        <h3>State</h3> 
        <p style="min-height:10px"></p>
        <p> In reinforcement learning, the state serves as the foundation for decision-making, providing the agent with crucial information about the environment. The state representation determines what aspects of the environment are observable to the agent and influences its ability to perceive, learn, and generalize across different situations. A well-designed state representation should capture relevant features of the environment while minimizing dimensionality and computational complexity. For my algorithm I studied 4 different states.
        <p style="min-height:10px"></p>
        </p>
            <ol>
                <li><strong>Full Information State Representation:</strong> In the full information approach, the state representation includes a binary grid indicating the occupancy of each cell on the Tetris board. It also includes information about the position, shape, and orientation of the falling Tetris block. This representation provides complete information about the current game state but may suffer from high dimensionality and scalability issues.</li>
                <p style="min-height:10px"></p>
                <li><strong>Plane Information State Representation:</strong> The plane information approach simplifies the state representation by providing information about the highest occupied plane in the placed blocks and the lowest occupied plane in the falling Tetris block. This representation aims to capture complementary patterns between the two planes to facilitate decision-making. It scales linearly with the width of the Tetris board.</li>
                <p style="min-height:10px"></p>
                <li><strong>Local Information State Representation:</strong> In the local information approach, the state representation focuses on the distance between each point in the falling Tetris block and the bottom floor of the board. It includes the number of points with the same distance and the shortest distance for each possible move. This representation allows the agent to have knowledge of the local space but limits its ability to plan multiple moves ahead. It is independent of the board size.</li>
                <p style="min-height:10px"></p>
                <li><strong>Minimal Information State Representation:</strong> The minimal information approach is a simplification of the local information method. It calculates the difference in the number of points with the same distance between the current state and the new state resulting from each possible action. The state consists of four values, one for each possible action, indicating the change in the number of points with the same distance. This representation aims to capture the immediate impact of each action on the local space.</li>
                </ol>

        <img src="img/portfolio/TetrisState.png" class="img-post" alt="{{ post.alt }}">
        <p style="min-height:50px"></p>

        <h3>State</h3> 
        <p style="min-height:10px"></p>
        <p>The different states were evaluated by looking at the average score after the learning process has concluded. The average score is affected by model parameters, for example
        </p>
        <p style="min-height:10px"></p>
         <ul>
                <li><strong>Learning Rate (Alpha):</strong> The learning rate, denoted as alpha, is a hyperparameter that controls the magnitude of updates to the model's parameters during training. It determines the size of the steps the algorithm takes when exploring the parameter space. A small learning rate may cause slow convergence and risk getting trapped in local minima, while a large learning rate may lead to unstable training and overshooting the optimal solution.</li>
                <p style="min-height:10px"></p>
                <li><strong>Discount Factor (Gamma):</strong> The discount factor, denoted as gamma, is specific to reinforcement learning and influences the model's prioritization of long-term rewards versus immediate rewards. Gamma values range between 0 and 1, with higher values indicating a greater emphasis on long-term strategy.</li>
                <p style="min-height:10px"></p>
                <li><strong>Exploration vs. Exploitation:</strong> During the initial stages of training, the algorithm employs exploration to randomly explore the state space and avoid local minima. Exploration allows the algorithm to discover diverse strategies and learn more about the environment. As the algorithm learns, it transitions towards exploitation, where it leverages the learned knowledge to make more informed decisions and exploit the most promising actions. Increasing the exploration step early in training can facilitate discovering optimal strategies and prevent premature convergence to suboptimal solutions.</li>
                <p style="min-height:10px"></p>
        </ul>
        <p>In the plot below the optimization process is shown for state number 3 (Local Information State Representation) converging into a learning rate of 0.001 and a discount factor of 0.9. With these values the model reached an average of 2.0 points per game and a record of 14 points.
        </p>
        <img src="img/portfolio/gamma_lr_plot.png" class="img-post" alt="{{ post.alt }}" width=400 height=auto>
        <p style="min-height:50px"></p>
        </div>
        
title2: Recognizing musical genre

description2: </p>
        <div class="cv-section">
        <p>As a longterm music fan I have always been interested in what makes a song a specific genre. As I was studying the application of ML (Machine Learning) on audio signals for my work, I decided to use some of the methods I had learned in a side project. My final aim is to understand whether it is possible for a machine to descern a user's taste using ML.</p>

        <p>I decided to begin with genre prediction and found an open dataset with 400 songs from 4 different genres which I used as my dataset and chose TensorFlow as my ML library. I initially attempted to feed waveform data directly into the algorithm. However, I encountered significant challenges due to slow performance and limitations in computational resources. Despite several attempts to optimize the algorithm, the computing power of my laptop proved insufficient to handle the complexity of the waveform data, hindering the training process.</p>

        <p>I decided to take a different approach, and my research into the subject brought me to MFCC (Mel-Frequency Cepstral Coefficients) analysis, a technique widely utilized in voice recognition and related tasks. Thanks to this change I was able to overcome my performanxe issues and the training process sped up significantly. I then proceeded to optimize my algorithm by carefully selecting parameters such as model layers, layer depth and learning rate. My final algorithm achieved a validation accuracy of 90% and ROI of 0.78. </p>

        <p>For future implementations of this algorithm I want to combine the two approaches. I will segment the original waveform in fractions that I will then use for the MFCC analysis, providing a change in frequencies over time.</p>

        <p><a href="https://github.com/matildaperuzzo" target="_blank">Github repository.</a></p>

        </div>
---